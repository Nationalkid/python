{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer Project 2017\n",
    "## Blowfly strike in rabbits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries\n",
    "(You will need to make sure that you pip install epydemiology package into your virtual Python environment before proceeding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import getpass\n",
    "import re, textwrap\n",
    "import socket\n",
    "import collections\n",
    "import epydemiology as epy\n",
    "import matplotlib\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the function that will download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phjGetSAVSNETData(phjQuery,\n",
    "                      phjPrintResults = True):\n",
    "    \n",
    "    # Make connection to MySQL database\n",
    "    # =================================\n",
    "    phjMaxAttempts = 3    # Max number of attmepts to make connection to database\n",
    "    \n",
    "    for i in range(phjMaxAttempts):\n",
    "        # phjServer = input(\"Enter host: \")\n",
    "        # phjServer = '138.253.152.243'\n",
    "        phjServer = 'savsnetnlp.vets.liv.ac.uk'\n",
    "        \n",
    "        phjUser = input(\"Enter MySQL database username: \")\n",
    "        phjPwd = getpass.getpass(\"Enter password: \")\n",
    "        \n",
    "        try:\n",
    "            phjConnection = pymysql.connect(host=phjServer, user=phjUser,password=phjPwd)\n",
    "            break    # Break to leap out of for loop\n",
    "            \n",
    "        except pymysql.err.OperationalError as e:\n",
    "            print(\"\\nAn OperationalError occurred. Error number {0}: {1}.\".format(e.args[0],e.args[1]))\n",
    "            \n",
    "            if i < (phjMaxAttempts-1):\n",
    "                print(\"\\nPlease try again.\\n\")    # Don't say 'Please try again' if last attempt has failed.\n",
    "            \n",
    "        except socket.timeout:\n",
    "            print(\"Timed out\")\n",
    "    \n",
    "    # Report that connection failed after maximum number of attempts and return None\n",
    "    if i == (phjMaxAttempts-1):\n",
    "        print(\"\\nFailed to make connection after {0} attempts.\".format(i+1))\n",
    "        return None\n",
    "    \n",
    "    # If connection has been made successfully...\n",
    "    else:\n",
    "        if phjPrintResults:\n",
    "            print(\"\\nQuery used to interrogate database =\\n\",phjQuery,\"\\n\")\n",
    "        \n",
    "        phjTempDF = pd.io.sql.read_sql(phjQuery, con=phjConnection)\n",
    "                    \n",
    "        phjConnection.close()\n",
    "\n",
    "        if phjPrintResults:\n",
    "            print('\\n')\n",
    "            print('Query completed')\n",
    "\n",
    "            print('Number of rows returned = ',len(phjTempDF.index))\n",
    "\n",
    "        return phjTempDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SQL query that will request the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query will download the complete Robovet narrative data. However, is has\n",
    "# been noted that the records included in the deidentified narrative table and\n",
    "# the table containing additional data are different. This is because the queries\n",
    "# used to extract each dataset were different. There was also an issue were\n",
    "# the query used to generate the table containing additional data did not include\n",
    "# many of the early records.\n",
    "# Consequently, to ensure the data read by students matches records that are\n",
    "# available to get additional information, the following query can be used.\n",
    "phjQuery = '''SELECT DISTINCT `RobovetConsultations_2017-05-19`.*,\n",
    "                              `deidentifiedRobovet`.`narrative` AS 'deidentifiedNarrative'\n",
    "              FROM `robovetDeidentified2017-01-30`.`RobovetConsultations_2017-05-19`\n",
    "                  LEFT JOIN `robovetDeidentified2017-01-30`.`deidentifiedRobovet`\n",
    "                      ON `RobovetConsultations_2017-05-19`.`consultID` = `deidentifiedRobovet`.`consultID`\n",
    "              WHERE (`deidentifiedRobovet`.`narrative` IS NOT NULL AND \n",
    "                     `deidentifiedRobovet`.`narrative` != \"\" AND\n",
    "                     `deidentifiedRobovet`.`narrative` != \"\\\"\\\"\")\n",
    "                  AND\n",
    "                    `RobovetConsultations_2017-05-19`.`species` = 'rabbit'\n",
    "            '''\n",
    "\n",
    "print(phjQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required data into a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = phjGetSAVSNETData(phjQuery, phjPrintResults = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out examples of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print('\\n')\n",
    "\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', 10):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the final version of the regex\n",
    "(N.B. There were a few typos in the original version (/s rather than \\s to indicate a space) which have been corrected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theFinalRegex = re.compile( r\"\"\"(?P<RabbitFS>\n",
    "                                (?<!disc.)\n",
    "                                (?<!cuss.)\n",
    "                                (?<!sion.)\n",
    "                                (?<!ssed.)\n",
    "                                (?<!vise.)\n",
    "                                (?<!rned.)   # Removes any variant of discussed, advise, warned, prevent, risk, about, of, from\n",
    "                                (?<!vent.)\n",
    "                                (?<!risk.)\n",
    "                                (?<!bout.)\n",
    "                                (?<!..of.)\n",
    "                                (?<!from.)\n",
    "                                (?<!s/s/no.)   # Removes no, and, re, for, avoid, diet, dietary, food, vaccination, \n",
    "                                (?<!.and.)\n",
    "                                (?<!..re.)\n",
    "                                (?<!.for.)\n",
    "                                (?<!void.)\n",
    "                                (?<!iet..)\n",
    "                                (?!<ing..)   # Removes any variant identified in the output data that was a false positive\n",
    "                                (?<!ant..)\n",
    "                                (?<!dary..)\n",
    "                                (?<!ood..)\n",
    "                                (?<!are..)\n",
    "                                (?<!vacc..)   \n",
    "                                (?<!.ion.)\n",
    "                                (?<!ase..)\n",
    "                                (?<!an..)\n",
    "                                (?<!nce..)\n",
    "                                (?<!rol..)   \n",
    "                                (?<!inst.)\n",
    "                                (?<!ware.)\n",
    "                                (?:fly.*strike|mag+ot+|my?ia?sis)\n",
    "                                (?!\\scontrol)(?!\\sprevent)(?!\\sprotect)) #Removes flys strike that has been followed by control, prevent, protect\n",
    "                             \"\"\", flags=re.I|re.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag the consultations where the regex finds a match\n",
    "**A new column called 'foundit' will be added to the dataframe and will contain either True or False depending on whether your regex found a match.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "print('\\n')\n",
    "\n",
    "if 'foundit' in df.columns:\n",
    "    df = df.drop('foundit',axis = 1)\n",
    "\n",
    "df['foundit'] =  df['deidentifiedNarrative'].str.contains(theFinalRegex)\n",
    "df['founditBinary'] = df['foundit'].astype(int)\n",
    "\n",
    "with pd.option_context('display.max_rows', 20, 'display.max_columns', 10):\n",
    "    print(df.loc[df['foundit']==True,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['consultMonth'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a series of new columns in the database, one for each capturing group in the regex.\n",
    "(Since there is only one group in the rabbit regex, this column will be the same as the 'foundit' column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract all matching strings from the consultations\n",
    "dfMatches =  df['deidentifiedNarrative'].str.extractall(theFinalRegex)\n",
    "\n",
    "# Define an empty dataframe\n",
    "dfLists = pd.DataFrame()\n",
    "\n",
    "# Concatenate multiple matches (over several rows) into a single list (on one row)\n",
    "for column in dfMatches:\n",
    "    dfLists[column] = dfMatches.groupby(level=0)[column].apply(list)\n",
    "\n",
    "# Remove columns if already exist\n",
    "columnNamesList = list(dfLists.columns.values)\n",
    "\n",
    "for col in columnNamesList:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(col,axis = 1)\n",
    "\n",
    "df = df.join(dfLists,how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable that shows number of weeks since start\n",
    "# --------------------------------------------------------\n",
    "# The consultWeek variable in the dataframe gives the week number in the year and returns to 1\n",
    "# at the beginning of each year. It also produces weeks that contain less tha 7 days at the\n",
    "# start and end of the year. Is there also a possibility that a year may contain 53 weeks?\n",
    "# Instead, to produce a variable that represents the number of weeks since a specific date\n",
    "# then calculate the number of days since the start followed by division by 7, stored as an int.\n",
    "\n",
    "df['consultDate'] = pd.to_datetime(df['consultDate'])\n",
    "\n",
    "phjStartDate = pd.to_datetime('2014-01-06')\n",
    "\n",
    "print(phjStartDate)\n",
    "\n",
    "df['days_since_start'] = (df['consultDate'] - phjStartDate).dt.days\n",
    "df['weeks_since_start'] = df['days_since_start'].divide(7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', 10):\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label consultations that have already been read and confirmed to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phjPrevReadConsultations = [114546,83451,85847,89505,2048740,93728,94355,149066,95415,99034,102413,104942,106778,108215,109743,450658,112293,120154,120916,123916,123926,124486,128396,136553,133457,135799,136903,159938,148063,153611,157777,160917,165891,165898,179656,184644,181223,193588,203170,232673,253041,425367,465311,390105,1932466,465708,521808,451686,469037,488289,2164286,511155,513561,514301,516756,525640,530185,534479,2048805,1675990,600421,602639,1795734,624409,646856,646930,653038,672228,1827616,1839680,2166400,2033838,2126000,2165245,2216207,1806567,908371,2168489,1874908,2063876,1718220,1947558,1286882,1932054,1992851,1789812,2168025,2317312,1611674,1628998,1630982,1635634,1637712,1861375,1646346,1646441,1655997,1665966,1668478,1674731,1824130,2060177,2094453,2128013,1709473,1710363,1710372,1762388,1769912,1771054,1780453,2335936,1787674,1782787,1791248,1795882,1952797,1798087,1807685,1828892,1843346,1859490,1857508,1858948,1877200,1865219,1866266,1868592,1878161,1889722,1895401,1930240,1955881,1895759,1897878,1925075,1900399,1903431,1906517,1918505,1924616,1927223,1930522,1941196,1947138,1950564,1956972,1956995,1966512,1990721,1992496,1996819,2012632,2013625,2321229,2376323,2071628,2096417,2032948,2056366,2045206,2045541,2046280,2049018,2058573,2076032,2067706,2097891,2082698,2083265,2084350,2085551,2088294,2089201,2093301,2094354,2106195,2100087,2105299,2111105,2115513,2135856,2142617,2142628,2143126,2148113,2148674,2148694,2148960,2150153,2171932,2151297,2151616,2154546,2164449,2170498,2178514,2179403,2184163,2188604,2194567,2202102,2214102,2218989,2244263,2244410,2253606,2256563,2278438,2305124,2365832,2385452,2497373,2504658,2618836]\n",
    "\n",
    "df['alreadyConfirmed'] = df['consultID'].isin(phjPrevReadConsultations)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe that summarises proportion of flystrike consults per week\n",
    "# ================================================================================\n",
    "# Aggregate based on week_since_start\n",
    "# -----------------------------------\n",
    "\n",
    "# N.B. This graph plots the proportion of consultations found by the regex.\n",
    "# You will need to adapt it to plot proportion of confirmed consultations.\n",
    "\n",
    "dfGroupbyWeekDF = df.groupby('weeks_since_start').agg({'days_since_start': min,\n",
    "                                                       'foundit': sum,\n",
    "                                                       'consultID': 'count'})\n",
    "\n",
    "# Reset the index - at the moment the weeks since start variable is the index\n",
    "dfGroupbyWeekDF = dfGroupbyWeekDF.reset_index()\n",
    "\n",
    "# Rename columns to something more meaningful\n",
    "dfGroupbyWeekDF = dfGroupbyWeekDF.rename(columns = {'days_since_start': 'days_since_start',\n",
    "                                                    'foundit': 'weeklyFlystrikeRegexMatches',\n",
    "                                                    'consultID': 'weeklyTotalConsults'})\n",
    "\n",
    "# Calculate weekly fireworks consultations per 1000 consultations\n",
    "# ---------------------------------------------------------------\n",
    "dfGroupbyWeekDF['weeklyFlystrikeRegexMatchesPer1000'] = (dfGroupbyWeekDF['weeklyFlystrikeRegexMatches']/dfGroupbyWeekDF['weeklyTotalConsults'])*1000\n",
    "\n",
    "# Print just the top and bottom rows for 3 columns only\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', 8):\n",
    "    print(dfGroupbyWeekDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot proportion of flystrike consultations\n",
    "dfGroupbyWeekDF.plot.line(x = 'weeks_since_start',\n",
    "                             y = 'weeklyFlystrikeRegexMatchesPer1000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export useful consultations to a CSV file, ready to be opened by Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only retain consultations where a regex match was found\n",
    "df = df.loc[df['foundit']==True,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you run this box, you will be presented with an input box\n",
    "# where you should enter the path and filename of the file you\n",
    "# want to save.\n",
    "#\n",
    "# Enter the path and filename of where to save the csv file.\n",
    "# On a Mac, the path will take the format:\n",
    "#     ~/Documents/Research_project/myFile.csv\n",
    "#\n",
    "# On Windows, the path will take the form:\n",
    "#     C:\\Documents\\Reserach_project\\myFile.csv\n",
    "\n",
    "pathToFile = input(\"Enter path and filename to save csv file: \")\n",
    "\n",
    "df.to_csv(pathToFile)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above code will produce a CSV file containing ONLY the consultations that match the regex together with some additional variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
